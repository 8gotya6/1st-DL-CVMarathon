{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 『本次練習內容』\n",
    "#### 運用這幾天所學觀念搭建一個CNN分類器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 『本次練習目的』\n",
    "  #### 熟悉CNN分類器搭建步驟與原理\n",
    "  #### 學員們可以嘗試不同搭法，如使用不同的Maxpooling層，用GlobalAveragePooling取代Flatten等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print(x_train.shape) #(50000, 32, 32, 3)\n",
    "\n",
    "## Normalize Data\n",
    "def normalize(X_train,X_test):\n",
    "        mean = np.mean(X_train,axis=(0,1,2,3))\n",
    "        std = np.std(X_train, axis=(0, 1, 2, 3))\n",
    "        X_train = (X_train-mean)/(std+1e-7)\n",
    "        X_test = (X_test-mean)/(std+1e-7) \n",
    "        return X_train, X_test,mean,std\n",
    "    \n",
    "    \n",
    "## Normalize Training and Testset    \n",
    "x_train, x_test,mean_train,std_train = normalize(x_train, x_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Coding\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "## OneHot Label 由(None, 1)-(None, 10)\n",
    "## ex. label=2,變成[0,0,1,0,0,0,0,0,0,0]\n",
    "one_hot=OneHotEncoder()\n",
    "y_train=one_hot.fit_transform(y_train).toarray()\n",
    "y_test=one_hot.transform(y_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0102 22:45:51.706843  9276 deprecation_wrapper.py:119] From C:\\Coding\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0102 22:45:51.724412  9276 deprecation_wrapper.py:119] From C:\\Coding\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0102 22:45:51.726399  9276 deprecation_wrapper.py:119] From C:\\Coding\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0102 22:45:51.760558  9276 deprecation_wrapper.py:119] From C:\\Coding\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0102 22:45:51.761501  9276 deprecation_wrapper.py:119] From C:\\Coding\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0102 22:45:53.331934  9276 deprecation_wrapper.py:119] From C:\\Coding\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0102 22:45:53.402717  9276 deprecation_wrapper.py:119] From C:\\Coding\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "C:\\Coding\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=100)`\n",
      "C:\\Coding\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=10)`\n",
      "W0102 22:45:53.536994  9276 deprecation_wrapper.py:119] From C:\\Coding\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0102 22:45:53.672678  9276 deprecation.py:323] From C:\\Coding\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 24s 484us/step - loss: 1.2707 - acc: 0.55981s - loss: 1\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.8177 - acc: 0.7111\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 23s 467us/step - loss: 0.5790 - acc: 0.7974\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 23s 455us/step - loss: 0.3893 - acc: 0.8664\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 23s 463us/step - loss: 0.2529 - acc: 0.9143\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 24s 485us/step - loss: 0.1722 - acc: 0.9410\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 24s 473us/step - loss: 0.1344 - acc: 0.9537\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 0.1118 - acc: 0.9620\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 25s 498us/step - loss: 0.1089 - acc: 0.9629\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 25s 509us/step - loss: 0.0944 - acc: 0.9671\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 25s 503us/step - loss: 0.0846 - acc: 0.9713\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 25s 509us/step - loss: 0.0673 - acc: 0.9768\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 26s 530us/step - loss: 0.0581 - acc: 0.9800\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 26s 524us/step - loss: 0.0699 - acc: 0.9769\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 26s 514us/step - loss: 0.0690 - acc: 0.9772\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 27s 530us/step - loss: 0.0697 - acc: 0.9771\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 25s 509us/step - loss: 0.0482 - acc: 0.9833\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 26s 513us/step - loss: 0.0428 - acc: 0.98580s - loss: 0.0425 - acc\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 26s 516us/step - loss: 0.0407 - acc: 0.98691s\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 26s 517us/step - loss: 0.0546 - acc: 0.9823\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 26s 515us/step - loss: 0.0556 - acc: 0.9825\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 25s 493us/step - loss: 0.0399 - acc: 0.9863\n",
      "Epoch 23/100\n",
      "50000/50000 [==============================] - 25s 496us/step - loss: 0.0309 - acc: 0.9898\n",
      "Epoch 24/100\n",
      "50000/50000 [==============================] - 24s 488us/step - loss: 0.0358 - acc: 0.9884\n",
      "Epoch 25/100\n",
      "50000/50000 [==============================] - 26s 513us/step - loss: 0.0382 - acc: 0.98701s - loss:\n",
      "Epoch 26/100\n",
      "50000/50000 [==============================] - 23s 462us/step - loss: 0.0403 - acc: 0.9867\n",
      "Epoch 27/100\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.0413 - acc: 0.9867\n",
      "Epoch 28/100\n",
      "50000/50000 [==============================] - 26s 525us/step - loss: 0.0316 - acc: 0.9897ETA: 3s  - ETA: 1s - lo - ETA: 0s - loss: 0.0317 - acc: 0.9\n",
      "Epoch 29/100\n",
      "50000/50000 [==============================] - 26s 515us/step - loss: 0.0363 - acc: 0.9884\n",
      "Epoch 30/100\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.0277 - acc: 0.9910\n",
      "Epoch 31/100\n",
      "50000/50000 [==============================] - 24s 486us/step - loss: 0.0274 - acc: 0.9913\n",
      "Epoch 32/100\n",
      "50000/50000 [==============================] - 25s 498us/step - loss: 0.0298 - acc: 0.9912\n",
      "Epoch 33/100\n",
      "50000/50000 [==============================] - 23s 468us/step - loss: 0.0365 - acc: 0.9878\n",
      "Epoch 34/100\n",
      "50000/50000 [==============================] - 20s 391us/step - loss: 0.0346 - acc: 0.9889\n",
      "Epoch 35/100\n",
      "50000/50000 [==============================] - 20s 408us/step - loss: 0.0234 - acc: 0.9920\n",
      "Epoch 36/100\n",
      "50000/50000 [==============================] - 22s 437us/step - loss: 0.0202 - acc: 0.9931\n",
      "Epoch 37/100\n",
      "50000/50000 [==============================] - 21s 423us/step - loss: 0.0355 - acc: 0.98850s - loss: 0.0356 - acc:\n",
      "Epoch 38/100\n",
      "50000/50000 [==============================] - 22s 443us/step - loss: 0.0303 - acc: 0.9902\n",
      "Epoch 39/100\n",
      "50000/50000 [==============================] - 21s 429us/step - loss: 0.0233 - acc: 0.9927\n",
      "Epoch 40/100\n",
      "50000/50000 [==============================] - 21s 420us/step - loss: 0.0239 - acc: 0.99280s - loss: 0.0242 - acc:\n",
      "Epoch 41/100\n",
      "50000/50000 [==============================] - 21s 411us/step - loss: 0.0263 - acc: 0.9915\n",
      "Epoch 42/100\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0210 - acc: 0.9931\n",
      "Epoch 43/100\n",
      "50000/50000 [==============================] - 19s 373us/step - loss: 0.0299 - acc: 0.9904\n",
      "Epoch 44/100\n",
      "50000/50000 [==============================] - 21s 428us/step - loss: 0.0221 - acc: 0.9928\n",
      "Epoch 45/100\n",
      "50000/50000 [==============================] - 21s 425us/step - loss: 0.0212 - acc: 0.99291s - loss\n",
      "Epoch 46/100\n",
      "50000/50000 [==============================] - 21s 427us/step - loss: 0.0214 - acc: 0.9932\n",
      "Epoch 47/100\n",
      "50000/50000 [==============================] - 21s 429us/step - loss: 0.0235 - acc: 0.9922\n",
      "Epoch 48/100\n",
      "50000/50000 [==============================] - 22s 442us/step - loss: 0.0266 - acc: 0.9922\n",
      "Epoch 49/100\n",
      "50000/50000 [==============================] - 22s 430us/step - loss: 0.0159 - acc: 0.9949\n",
      "Epoch 50/100\n",
      "50000/50000 [==============================] - 20s 398us/step - loss: 0.0149 - acc: 0.9953\n",
      "Epoch 51/100\n",
      "50000/50000 [==============================] - 19s 386us/step - loss: 0.0257 - acc: 0.9925\n",
      "Epoch 52/100\n",
      "50000/50000 [==============================] - 19s 387us/step - loss: 0.0254 - acc: 0.9919\n",
      "Epoch 53/100\n",
      "50000/50000 [==============================] - 22s 432us/step - loss: 0.0194 - acc: 0.9938\n",
      "Epoch 54/100\n",
      "50000/50000 [==============================] - 23s 455us/step - loss: 0.0170 - acc: 0.99480s - loss: 0.0165\n",
      "Epoch 55/100\n",
      "50000/50000 [==============================] - 22s 444us/step - loss: 0.0215 - acc: 0.9933\n",
      "Epoch 56/100\n",
      "50000/50000 [==============================] - 22s 443us/step - loss: 0.0162 - acc: 0.9946\n",
      "Epoch 57/100\n",
      "50000/50000 [==============================] - 22s 442us/step - loss: 0.0146 - acc: 0.9954\n",
      "Epoch 58/100\n",
      "50000/50000 [==============================] - 21s 424us/step - loss: 0.0182 - acc: 0.9946\n",
      "Epoch 59/100\n",
      "50000/50000 [==============================] - 19s 388us/step - loss: 0.0207 - acc: 0.99351s - loss:\n",
      "Epoch 60/100\n",
      "50000/50000 [==============================] - 19s 383us/step - loss: 0.0164 - acc: 0.9950\n",
      "Epoch 61/100\n",
      "50000/50000 [==============================] - 22s 449us/step - loss: 0.0165 - acc: 0.9951\n",
      "Epoch 62/100\n",
      "50000/50000 [==============================] - 22s 435us/step - loss: 0.0168 - acc: 0.9950\n",
      "Epoch 63/100\n",
      "50000/50000 [==============================] - 22s 449us/step - loss: 0.0225 - acc: 0.9939\n",
      "Epoch 64/100\n",
      "50000/50000 [==============================] - 22s 437us/step - loss: 0.0162 - acc: 0.9952\n",
      "Epoch 65/100\n",
      "50000/50000 [==============================] - 22s 439us/step - loss: 0.0136 - acc: 0.9956\n",
      "Epoch 66/100\n",
      "50000/50000 [==============================] - 22s 446us/step - loss: 0.0176 - acc: 0.9946\n",
      "Epoch 67/100\n",
      "50000/50000 [==============================] - 26s 520us/step - loss: 0.0197 - acc: 0.9940\n",
      "Epoch 68/100\n",
      "50000/50000 [==============================] - 26s 518us/step - loss: 0.0207 - acc: 0.9941\n",
      "Epoch 69/100\n",
      "50000/50000 [==============================] - 28s 553us/step - loss: 0.0188 - acc: 0.9943\n",
      "Epoch 70/100\n",
      "50000/50000 [==============================] - 27s 531us/step - loss: 0.0134 - acc: 0.9956\n",
      "Epoch 71/100\n",
      "50000/50000 [==============================] - 26s 516us/step - loss: 0.0098 - acc: 0.9967\n",
      "Epoch 72/100\n",
      "50000/50000 [==============================] - 26s 522us/step - loss: 0.0108 - acc: 0.99651s\n",
      "Epoch 73/100\n",
      "50000/50000 [==============================] - 26s 510us/step - loss: 0.0135 - acc: 0.9955\n",
      "Epoch 74/100\n",
      "50000/50000 [==============================] - 25s 508us/step - loss: 0.0185 - acc: 0.9943\n",
      "Epoch 75/100\n",
      "50000/50000 [==============================] - 25s 509us/step - loss: 0.0129 - acc: 0.9962\n",
      "Epoch 76/100\n",
      "50000/50000 [==============================] - 26s 523us/step - loss: 0.0130 - acc: 0.9962\n",
      "Epoch 77/100\n",
      "50000/50000 [==============================] - 25s 496us/step - loss: 0.0181 - acc: 0.9947\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 25s 491us/step - loss: 0.0127 - acc: 0.9960\n",
      "Epoch 79/100\n",
      "50000/50000 [==============================] - 25s 505us/step - loss: 0.0105 - acc: 0.9966\n",
      "Epoch 80/100\n",
      "50000/50000 [==============================] - 25s 497us/step - loss: 0.0144 - acc: 0.9953\n",
      "Epoch 81/100\n",
      "50000/50000 [==============================] - 25s 503us/step - loss: 0.0137 - acc: 0.9962\n",
      "Epoch 82/100\n",
      "50000/50000 [==============================] - 25s 508us/step - loss: 0.0099 - acc: 0.9966\n",
      "Epoch 83/100\n",
      "50000/50000 [==============================] - 25s 501us/step - loss: 0.0125 - acc: 0.99621s - loss: 0.0124 - acc:  - ETA: 0s - loss: 0.0125 - a\n",
      "Epoch 84/100\n",
      "50000/50000 [==============================] - 23s 455us/step - loss: 0.0177 - acc: 0.9940\n",
      "Epoch 85/100\n",
      "50000/50000 [==============================] - 24s 475us/step - loss: 0.0082 - acc: 0.9971\n",
      "Epoch 86/100\n",
      "50000/50000 [==============================] - 26s 518us/step - loss: 0.0071 - acc: 0.9975\n",
      "Epoch 87/100\n",
      "50000/50000 [==============================] - 25s 502us/step - loss: 0.0217 - acc: 0.9939\n",
      "Epoch 88/100\n",
      "50000/50000 [==============================] - 26s 518us/step - loss: 0.0194 - acc: 0.99451s - loss\n",
      "Epoch 89/100\n",
      "50000/50000 [==============================] - 26s 518us/step - loss: 0.0110 - acc: 0.9965\n",
      "Epoch 90/100\n",
      "50000/50000 [==============================] - 26s 521us/step - loss: 0.0072 - acc: 0.9977\n",
      "Epoch 91/100\n",
      "50000/50000 [==============================] - 26s 523us/step - loss: 0.0069 - acc: 0.9977\n",
      "Epoch 92/100\n",
      "50000/50000 [==============================] - 27s 534us/step - loss: 0.0123 - acc: 0.99641s - loss: 0\n",
      "Epoch 93/100\n",
      "50000/50000 [==============================] - 27s 534us/step - loss: 0.0196 - acc: 0.9944\n",
      "Epoch 94/100\n",
      "50000/50000 [==============================] - 26s 520us/step - loss: 0.0128 - acc: 0.9959\n",
      "Epoch 95/100\n",
      "50000/50000 [==============================] - 27s 532us/step - loss: 0.0092 - acc: 0.9972\n",
      "Epoch 96/100\n",
      "50000/50000 [==============================] - 25s 501us/step - loss: 0.0083 - acc: 0.9976\n",
      "Epoch 97/100\n",
      "50000/50000 [==============================] - 26s 517us/step - loss: 0.0102 - acc: 0.9966\n",
      "Epoch 98/100\n",
      "50000/50000 [==============================] - 27s 533us/step - loss: 0.0135 - acc: 0.9960\n",
      "Epoch 99/100\n",
      "50000/50000 [==============================] - 26s 522us/step - loss: 0.0155 - acc: 0.9956\n",
      "Epoch 100/100\n",
      "50000/50000 [==============================] - 26s 524us/step - loss: 0.0078 - acc: 0.9976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x221661a1e10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "classifier=Sequential()\n",
    "\n",
    "#卷積組合\n",
    "classifier.add(Convolution2D(input_shape=(32, 32, 3), kernel_size=(3,3), filters=32, activation='relu', padding='same'))#32,3,3,input_shape=(32,32,3),activation='relu''\n",
    "classifier.add(BatchNormalization())\n",
    "\n",
    "'''自己決定MaxPooling2D放在哪裡'''\n",
    "#classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#卷積組合\n",
    "classifier.add(Convolution2D(kernel_size=(3,3), filters=32, activation='relu', padding='same'))\n",
    "classifier.add(BatchNormalization())\n",
    "\n",
    "#flatten\n",
    "classifier.add(Flatten())\n",
    "\n",
    "#FC\n",
    "classifier.add(Dense(output_dim=100, activation='relu')) #output_dim=100,activation=relu\n",
    "\n",
    "#輸出\n",
    "classifier.add(Dense(output_dim=10,activation='softmax'))\n",
    "\n",
    "#超過兩個就要選categorical_crossentrophy\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "classifier.fit(x_train,y_train,batch_size=100,epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 預測新圖片，輸入影像前處理要與訓練時相同\n",
    "#### ((X-mean)/(std+1e-7) ):這裡的mean跟std是訓練集的\n",
    "## 維度如下方示範"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.9973771e-12, 3.7319681e-16, 2.5015012e-07, 9.9999976e-01,\n",
       "        7.9859319e-09, 4.9278691e-13, 8.0610529e-10, 9.8268919e-13,\n",
       "        1.1764693e-14, 8.6518770e-21]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_example=(np.zeros(shape=(1,32,32,3))-mean_train)/(std_train+1e-7) \n",
    "classifier.predict(input_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow-gpu)",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
